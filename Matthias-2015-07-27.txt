Started in the 30s with Church.

Godel has a language that can prove anything.

Church says you can prove anything with functions.

Set notation was `^ x | x¬≤ < 0`, church annotated `'^ | x¬≤ < 0`, which was
typeset as `Œª | x¬≤ < 0`.

    e ‚©¥ x | Œªx.e | e e

See things as trees:

    ((Œªx.x)(Œªx.x)) = (Œª.‚Üê) (Œª.‚Üê)

The names don't matter, the meaning of variable is its binding-site.

    Œªdavid. (david david) = Œªjeff. (jeff jeff)

"`Œª` is just 7th-grade algebra hyped up a little bit."

    f(x) ‚âî x¬≤

    f(6) + 5 = 6¬≤ + 5 = 41

    (Œªx.e)e' = e[x‚Üêe'] -- Œ≤

    (Œªx.x) a = a
  
    (Œªxy.xyy)ab = (Œªy.ayy)b = abb

Lambdas are not functions! What about partial functions?

`Œ≤` is a relation, and requires a `Œª` on the outside.

    Œªx.Œªy.Œªz.‚∏§(Œªx.x)(zz)‚∏•y

We can't reduce with `Œ≤`.

`Œ≤` is a "notion of reduction".

We start defining `=‚∏§Œ≤‚∏£`:

    e Œ≤ e'
    ---------
    e =‚∏§Œ≤‚∏• e'

    e =‚∏§Œ≤‚∏• e'
    ---------------
    Œªx.e =‚∏§Œ≤‚∏• Œªx.e'

Template structure is called a "redex". 

["redex" is not latin, it just means reducible expression. "contractum" is
latin.]

Need another set of rules..

    e =‚∏§Œ≤‚∏• e'
    ----------------
    e‚ÇÄ e =‚∏§Œ≤‚∏• e‚ÇÄ e'

    e =‚∏§Œ≤‚∏• e'
    ----------------
    e e‚ÇÄ =‚∏§Œ≤‚∏• e' e‚ÇÄ

We are creating a "syntactic compatibility closure".

We also need the reflexive, symmetric and transitive closure of `=‚∏§Œ≤‚∏•`.

This gives an equivalence relation.

We have "a system of calculating equivalences between terms".

Q: "does something have meaning?"

Two possible meanings:

1. "Can you prove 'true = false', or 'is everything related'"
   You need to prove (meta-proof), that you cannot prove (in the system), that
   some two terms are equal.
   This is called a consistency theorem, developed by Church+Rosser, "the
   Church+Rosser lemma".
   This shows that the system relates some terms, but not all terms.
2. Is there a topologically, algebraically generated space of functions
   generated by `Œª` and satisfying `=‚∏§Œ≤‚∏•`.
   This was worked out by Dana Scott.

"lambda-calculus and denotational semantics had a terrible influence on
computer science" --MF

1958: Lisp and Algol 60 were created.

Lisp: 
- introduced `Œª`-notation, got it wrong

Algol 60: 
- based on substitution model of `Œª`-calculus
- call-by-name parameter passing (`Œ≤`-rule) (was very slow)
- then also introduced call-by-value
- cvn vs cbv "one was correct, one was fast"

For the next 15 years, people struggled to relate call-by-name (correct) with
call-by-value (fast).

Landin (1960s, '62, '63), invented the idea of abstract syntax.
Bohm did the same thing.
McCarthy tried something similar.

Abelson and Sussman make popular "applicative order application".

Dana Scott assigned a mathematical meaning to `Œª`-calculus:
1. Created the function space
2. Assigning a mapping from `Œª ‚Üí ‚ü¶‚üß`

MF opinion, denotational semantics took us off track.

Plotkin solved all of this (1972/1974) "Call-by-name, call-by-value and the
lambda calculus". Launched enough research ideas to fill 15 people's entire
research lives. Read this paper it's really good!!!

Gives an algorithm to understand what a calculus and a semantics is for a
programming language (13 steps?).

Launched research into CPS.

1. Pick a term language, scoped
2. Pick a subset of terms, called programs, and another subset, called values
   (first appearance of words 'program' and 'value' in study of `Œª` up to that
   point.)
   - Programs are things we don't really know what to do with immediately
   - Values are things "you see" at the end of computation. `Œª` is a value.

                .- input
         (Œªi.e) e' ~~~~~~> output
         -----
         ^ program proper

3. Define a notion of reduction: `Œ≤` and `Œ≤-value`

       Œ≤·µ•: (Œªx.e)v ~> e[x‚Üêv]

4. Uniformly crate a calculus `=‚Çì` from the notions of reduction.

       `=‚Çô` from Œ≤ and `=·µ•` from Œ≤ and Œ≤·µ•

   A way to equating arbitrary program fragments.

5. Define a semantics from `=‚Çì`

       eval‚Çì ‚àà ùí´(Program √ó Value)

       e eval‚Çì v ùëñùëìùëì e =‚Çì x

6. Prove that `eval‚Çì` is a function.

   Via Church-Rosser Lemma, `evalÀ£` is a (partial) function

       eval‚Çì(e) ‚âî { n          for "base" value 
                  | 'closure   for Œª-expression }

   You can now prove things like:
   
       e (Y e) =‚Çô Y e
   
   Computation should be directed, which for now is not specified, and
   problematic.

7. Prove that `=‚Çì` satisfies a "standardization" property:

       ùëñùëì  e =‚Çì e' ùë°‚Ñéùëíùëõ then you can do so in an algorithmic fashion

   An algorithm means you know how to pick the next redex.

   The algorithm is the same for CBN and CBV.

   "left-most outer-most strategy", ùëñ.ùëí. standard reduction. `|-->‚Çì`.

   A strategy is a meta-function for picking a redex.

   "If all you care about is the value at the end, you can use standard
   reduction".

       eval‚Çì(e) = v ùëñùëìùëì e |-->‚Çì* v

   Proof in Curry and Fays, Curry Fays theorem.

(Aside: you must give readers a guide for how to pronounce math notation! A
reader should be able to read your paper aloud.)

We have two semantics, `eval‚Çì` based on standard reduction, and `=‚Çì` based on
equality.

Must prove that `eval‚ÇìSR` is the same function as `eval‚Çì=`.

CBN calculus inconsistent with CBV interpreter, CBV calculus inconsistent with
CBN interpreter.

What do calculations on program mean?

1. (Syntactic) because you prove Church/Rosser, you know that calculations are
   consistent with the "fast" interpreter
2. (Semantic) via snippet from Jim Morris (63) dissertation, created
   polymorphic lambda calculus (PAL): introduce a relation known as
   observational equivalence.

   `e ‚âÉ e'` means for all ways of placing a term into a complete program (a
   context) called C, eval‚Çì(C[e]) ~ eval‚Çì(C[e'])

   Two versions: `‚âÉ‚Çô` and `‚âÉ·µ•`. These are the largest possible consistent
   equivalence relations that let you calculate programs. Therefore they are
   unique (because they are larges). They are the _truth_.

   Every programming language has "the truth" (`‚âÉ‚Çô`) by virtue of having an
   interpreter. The goal is to make the proof system (`=‚Çì`) consistent with the
   truth.

   MF: "On the expressive power of programming languages", previous draft
   attempted to prove `‚âÉ·µ• ‚äÜ  ‚âÉ‚Çô`, was proved different two months earlier.

CBV and CBN functional programming are not related other than in the syntax of
the terms. CBN is not "a different strategy".

Laziness and CBN are related, by subset.

Q: what use is studying functional programming if programs aren't purely
functional?

    (f (call/cc g)) ~ g(f)

A calculus equation for a very imperative idea.

Technical insights: "evaluation context semantics" _use contexts instead of
inference rules_.

    e Œ≤ e'
    ---------    <-- inference rule
    e =‚∏§Œ≤‚∏• e'

"Syntactic compatibility"

"left-most-outer-most"

Contexts:

    e ‚©¥ x | Œªx.e | e e
    C ‚©¥ ‚ñ° | Œªx.C | C e | e C

one-hole contexts.

`C[e]` "textually" put `e` into hole.

    (Œªx.‚ñ°)(Œªy.y)
          
     / \
    Œª  Œª‚Üê‚Üê
    |  | ‚Üë
    ‚ñ°  ‚ãÖ‚Üí‚Üí

with contexts:

    =‚∏§Œ≤‚∏• : e =‚∏§Œ≤‚∏• e' ùëñùëìùëì  ‚àÉ C,
         e  = C[(Œªx.e‚ÇÄ)e‚ÇÅ]
         e' = C[e‚ÇÄ[x ‚Üêe‚ÇÅ]]

Evaluation context:

    E ‚©¥ ‚ñ° | E e

Thm: E[(Œªx.e)e'] is the LMOM redex.

For CBV you need:

    E ‚©¥ ‚ñ° | v E | E e

You could also use:

    E ‚©¥ ‚ñ° | e E | E v

also left-most-outer-most!

    E[(Œªx.e)e'] |-->‚Çô E[e[x‚Üêe']]

fully describes CBN standard reduction.

    E[(Œªx.e)v] |-->·µ• E[e[x‚Üêv]]

fully describes CBV standard reduction.

"evaluation context semantics" should be called "standard reduction semantics".

Technical Insight 2:

    E[ THING v ]

`THING` can manipulate `E`, the evaluation context.

From this you can do side-effects, continuations, etc.

ùëí.ùëî.

    E[raise e] ~> raise e

full equational system for exceptions:

    x | Œªx.e | ee | raise e

calculation system:

    C[(Œªx.e)v]    =‚Çë‚Çì C[e[x‚Üêv]]
    C[E[raise e]] =‚Çë‚Çì C[raise e]

These two rules give you a consistent Church/Rosser system for exceptions. Same
two equations work for CBN.

Standard reduction:

    E[(Œªx.e)v]     |-->‚Çë‚Çì E[e[x‚Üêv]]
    E[E'[raise e]] |-->‚Çë‚Çì E[raise e]   [ |-->‚Çë‚Çì raise e , as a coincidence  ]

Standard reduction for assignment:

    e = x | Œªx.e | e e | set! x e | letrec ((x v) ..) e
    v = Œªx.e

    E = ‚ñ° | E e | v E | set! x E

    (Œ≤‚Çõ‚Çë‚Çú):  (Œªx.e) v                          R  letrec ((x v)) e
    (x):     letrec (.. (x v) ..) E[x]         R  letrec (.. (x v) ..) E[v]
    (set!):  letrec (.. (x v) ..) E[set! x u]  R  letrec (.. (x u) ..) E[Œªx.x]

    (scope extrusion:)
      E[letrec (...) e]  R  letrec (...) E[e]

    (merge:)
      letrec (.. (x v) ..) (letrec (.. (y u) ..) e)  R  letrec (.. (x v) .. .. (y u) ..) e

You can calculate in parallel, but standard reduction doesn't capture parallel
execution.

Technical Insight 3:

    t:   E[(Œªx.e)e']   = P‚Çú
    t+1: E[e[x‚Üêe']]    = P‚Çú‚Çä‚ÇÅ

Idea: separate `E` from the expression where the "machine" is looking for a
redex.

Two register machine: control and stack registers:

    ‚ü®e,E‚ü©

Next idea: change data representation from context to stack:

    ‚ü®e,[app‚ÇÅ]‚ü©
       [app‚ÇÇ]
       [app‚ÇÉ]
        ...

Next idea: substitution is hard and inefficient. Make substitution lazy;
reveals an explicit environment.

    control:     e
    environment: œÅ  mapping free-variables to values
    stack:       Œ∫  control stack


